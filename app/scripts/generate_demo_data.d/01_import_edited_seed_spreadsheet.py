#!/usr/bin/env python3
"""
Import demo data from Excel spreadsheet into PostgreSQL demo schema (v2).

This script reads the Excel file generated by generate_demo_data_excel_v2.py
and imports it into the demo schema, completely replacing existing data.

Version 2 Features:
- Handles pending discovery records (missing FQDNs, _PT_PENDING_ values)
- Properly imports records that rely on database defaults
- Provides data quality statistics in the summary

Features:
- Complete demo schema nuking (TRUNCATE with CASCADE)
- Database introspection for schema discovery
- Bulk insert for performance
- Foreign key validation
- Transaction safety (all or nothing)
- Detailed import summary

Usage:
    python import_demo_data_excel.py [--file path/to/excel.xlsx]
    
    Arguments:
        --file: Path to Excel file (default: latest in spreadsheets.d/)
        --confirm: Skip confirmation prompt for automation
        
Environment:
    Requires .env file with database credentials:
    - POSTGRESQL_INSTANCE_HOST
    - POSTGRESQL_INSTANCE_PORT
    - POSTGRESQL_BAIS_DB
    - POSTGRESQL_BAIS_DB_ADMIN_USER
    - POSTGRESQL_BAIS_DB_ADMIN_PASSWORD
"""

import os
import sys
import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import psycopg2
from psycopg2.extras import execute_batch, RealDictCursor
import pandas as pd
from dotenv import load_dotenv
from tabulate import tabulate


class DemoDataImporter:
    """Import demo data from Excel into PostgreSQL demo schema."""
    
    def __init__(self, excel_path: Optional[str] = None):
        """Initialize importer with database connection."""
        load_dotenv()
        
        # Database credentials from environment
        self.db_config = {
            'host': os.getenv('POSTGRESQL_INSTANCE_HOST', 'localhost'),
            'port': os.getenv('POSTGRESQL_INSTANCE_PORT', '5432'),
            'database': os.getenv('POSTGRESQL_BAIS_DB', 'prutech_bais'),
            'user': os.getenv('POSTGRESQL_BAIS_DB_ADMIN_USER'),
            'password': os.getenv('POSTGRESQL_BAIS_DB_ADMIN_PASSWORD')
        }
        
        if not self.db_config['user'] or not self.db_config['password']:
            raise ValueError("Database credentials not found in environment. Please check .env file.")
        
        # Find Excel file
        self.excel_path = self._find_excel_file(excel_path)
        if not self.excel_path.exists():
            raise FileNotFoundError(f"Excel file not found: {self.excel_path}")
        
        # Load column mappings
        mappings_path = Path(__file__).parent.parent.parent / 'configs' / 'excel_spreadsheet_column_mappings.json'
        with open(mappings_path, 'r') as f:
            self.column_mappings = json.load(f)
        
        # Statistics
        self.stats = {
            'tables_truncated': [],
            'rows_imported': {},
            'errors': [],
            'warnings': []
        }
    
    def _find_excel_file(self, excel_path: Optional[str]) -> Path:
        """Find Excel file to import."""
        if excel_path:
            return Path(excel_path)
        
        # Look for latest file in spreadsheets.d/
        spreadsheets_dir = Path(__file__).parent.parent / 'spreadsheets.d'
        excel_files = list(spreadsheets_dir.glob('editable_seed_spreadsheet_*.xlsx'))
        print(spreadsheets_dir)
        
        if not excel_files:
            raise FileNotFoundError(f"No Excel files found in {spreadsheets_dir}")
        
        # Return most recent file
        return max(excel_files, key=lambda f: f.stat().st_mtime)
    
    def connect(self) -> psycopg2.extensions.connection:
        """Create database connection."""
        return psycopg2.connect(**self.db_config)
    
    def introspect_schema(self, conn: psycopg2.extensions.connection) -> Dict[str, List[Dict]]:
        """Introspect demo schema structure."""
        schema_info = {}
        
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            # Get all tables in demo schema
            cur.execute("""
                SELECT table_name
                FROM information_schema.tables
                WHERE table_schema = 'demo'
                  AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """)
            tables = [row['table_name'] for row in cur.fetchall()]
            
            # Get column info for each table
            for table in tables:
                cur.execute("""
                    SELECT 
                        column_name,
                        data_type,
                        is_nullable,
                        column_default,
                        ordinal_position
                    FROM information_schema.columns
                    WHERE table_schema = 'demo' 
                      AND table_name = %s
                    ORDER BY ordinal_position
                """, (table,))
                schema_info[table] = list(cur.fetchall())
        
        return schema_info
    
    def nuke_demo_schema(self, conn: psycopg2.extensions.connection) -> None:
        """Completely clear ALL data from demo schema."""
        print("\nüóëÔ∏è  Nuking entire demo schema...")
        
        with conn.cursor() as cur:
            # Truncate all tables in dependency order
            all_tables = [
                'biz_component_relationships',
                'biz_components',
                'component_subtypes',
                'component_types',
                'component_ops_statuses',
                'component_environments',
                'component_physical_locations',
                'component_abstraction_levels',
                'component_relationship_types',
                'component_protocols'
            ]
            
            for table in all_tables:
                try:
                    cur.execute(f"TRUNCATE TABLE demo.{table} CASCADE")
                    self.stats['tables_truncated'].append(table)
                    print(f"  ‚úì Truncated {table}")
                except Exception as e:
                    self.stats['warnings'].append(f"Could not truncate {table}: {e}")
            
            conn.commit()
            print(f"  Cleared {len(self.stats['tables_truncated'])} tables")
            return
            
            # OLD CODE - keeping for reference
            cur.execute("""
                WITH RECURSIVE fk_tree AS (
                    -- Tables with no foreign keys
                    SELECT 
                        c.relname AS table_name,
                        0 AS level
                    FROM pg_class c
                    JOIN pg_namespace n ON n.oid = c.relnamespace
                    WHERE n.nspname = 'demo'
                      AND c.relkind = 'r'
                      AND NOT EXISTS (
                          SELECT 1 FROM pg_constraint
                          WHERE conrelid = c.oid AND contype = 'f'
                      )
                    
                    UNION ALL
                    
                    -- Tables with foreign keys
                    SELECT 
                        c.relname AS table_name,
                        ft.level + 1
                    FROM pg_class c
                    JOIN pg_namespace n ON n.oid = c.relnamespace
                    JOIN pg_constraint con ON con.conrelid = c.oid
                    JOIN pg_class ref ON ref.oid = con.confrelid
                    JOIN fk_tree ft ON ft.table_name = ref.relname
                    WHERE n.nspname = 'demo'
                      AND c.relkind = 'r'
                      AND con.contype = 'f'
                )
                SELECT table_name 
                FROM (
                    SELECT DISTINCT table_name, MAX(level) as max_level
                    FROM fk_tree
                    GROUP BY table_name
                ) t
                ORDER BY max_level DESC, table_name
            """)
            
            tables = [row[0] for row in cur.fetchall()]
            
            # TRUNCATE all tables with CASCADE
            for table in tables:
                try:
                    cur.execute(f"TRUNCATE TABLE demo.{table} CASCADE")
                    self.stats['tables_truncated'].append(table)
                    print(f"  ‚úì Truncated {table}")
                except Exception as e:
                    self.stats['warnings'].append(f"Could not truncate {table}: {e}")
        
        conn.commit()
        print(f"  Cleared {len(self.stats['tables_truncated'])} tables")
    
    def read_excel_data(self) -> Dict[str, pd.DataFrame]:
        """Read all sheets from Excel file."""
        print(f"\nüìñ Reading Excel file: {self.excel_path.name}")
        
        excel_data = {}
        xl_file = pd.ExcelFile(self.excel_path)
        
        for sheet_name in xl_file.sheet_names:
            if sheet_name in ['Summary', 'Issues', 'Reference_Tables']:
                continue  # Skip non-data sheets
            
            df = pd.read_excel(xl_file, sheet_name=sheet_name)
            
            # Handle timezone-aware datetime columns
            for col in df.columns:
                if pd.api.types.is_datetime64_any_dtype(df[col]):
                    df[col] = pd.to_datetime(df[col]).dt.tz_localize(None)
            
            excel_data[sheet_name] = df
            print(f"  ‚úì Read {sheet_name}: {len(df)} rows")
        
        return excel_data
    
    def get_column_mapping(self, table_name: str) -> Dict[str, str]:
        """Get Excel to database column mapping for a table."""
        if table_name in self.column_mappings:
            # Reverse mapping: Excel name -> DB name
            return {v: k for k, v in self.column_mappings[table_name].items()}
        return {}
    
    def ensure_reference_data(self, conn: psycopg2.extensions.connection,
                            excel_data: Dict[str, pd.DataFrame]) -> None:
        """Ensure reference data exists - from Excel or from live schema."""
        print("\nüì• Setting up reference data...")
        
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            # Check if we have reference data in Excel
            has_excel_refs = any(table in excel_data for table in [
                'component_types', 'component_environments', 'component_physical_locations'
            ])
            
            if has_excel_refs:
                print("  Using reference data from Excel file")
                self.import_reference_tables_from_excel(conn, excel_data)
            else:
                # Copy from live schema
                print("  Copying reference data from live schema")
                cur.execute("SELECT COUNT(*) as count FROM live.component_types")
                if cur.fetchone()['count'] == 0:
                    print("  Warning: No reference data in live schema either")
                    self.stats['warnings'].append("No reference data found in live schema")
                    return
                
                reference_tables = [
                    'component_types',
                    'component_subtypes',
                    'component_ops_statuses',
                    'component_environments',
                    'component_physical_locations',
                    'component_abstraction_levels',
                    'component_relationship_types',
                    'component_protocols'
                ]
                
                for table in reference_tables:
                    cur.execute(f"""
                        INSERT INTO demo.{table}
                        SELECT * FROM live.{table}
                        ON CONFLICT DO NOTHING
                    """)
                    cur.execute(f"SELECT COUNT(*) as count FROM demo.{table}")
                    count = cur.fetchone()['count']
                    print(f"  ‚úì {table}: {count} rows")
                
                conn.commit()
    
    def import_reference_tables_from_excel(self, conn: psycopg2.extensions.connection, 
                              excel_data: Dict[str, pd.DataFrame]) -> None:
        """Import reference/lookup tables from Excel."""
        # Order matters for foreign keys
        reference_tables = [
            'component_types',
            'component_subtypes',
            'component_ops_statuses',
            'component_environments',
            'component_physical_locations',
            'component_abstraction_levels',
            'component_relationship_types',
            'component_protocols'
        ]
        
        with conn.cursor() as cur:
            for table in reference_tables:
                if table not in excel_data:
                    continue
                
                df = excel_data[table]
                if df.empty:
                    continue
                
                # Get column mapping
                mapping = self.get_column_mapping(table)
                
                # Prepare data for insertion
                columns = list(df.columns)
                db_columns = [mapping.get(col, col) for col in columns]
                
                # Build INSERT query
                placeholders = ', '.join(['%s'] * len(columns))
                columns_str = ', '.join(db_columns)
                query = f"""
                    INSERT INTO demo.{table} ({columns_str})
                    VALUES ({placeholders})
                    ON CONFLICT DO NOTHING
                """
                
                # Convert DataFrame to list of tuples
                rows = df.where(pd.notnull(df), None).to_records(index=False).tolist()
                
                # Bulk insert
                execute_batch(cur, query, rows, page_size=100)
                self.stats['rows_imported'][table] = len(rows)
                print(f"  ‚úì Imported {table}: {len(rows)} rows")
        
        conn.commit()
    
    def import_components(self, conn: psycopg2.extensions.connection,
                         excel_data: Dict[str, pd.DataFrame]) -> None:
        """Import business components, handling pending discovery records."""
        print("\nüì• Importing business components...")

        table = 'biz_components'
        if table not in excel_data:
            self.stats['warnings'].append(f"No {table} sheet found in Excel")
            return

        df = excel_data[table]
        if df.empty:
            self.stats['warnings'].append(f"No data in {table} sheet")
            return

        # Get column mapping
        mapping = self.get_column_mapping(table)

        # Track data quality
        quality_counts = {'_PT_GREEN_RECORD_': 0, '_PT_YELLOW_RECORD_': 0, '_PT_RED_RECORD_': 0}
        pending_fqdn_count = 0

        with conn.cursor() as cur:
            # Process each row individually to handle missing FQDNs
            for idx, row in df.iterrows():
                # Check if FQDN is missing (use DB default)
                if pd.isna(row.get('fqdn')) or row.get('fqdn') == '':
                    # Build INSERT without FQDN column
                    columns_without_fqdn = [col for col in df.columns if col != 'fqdn']
                    db_columns = [mapping.get(col, col) for col in columns_without_fqdn]
                    placeholders = ', '.join(['%s'] * len(columns_without_fqdn))
                    columns_str = ', '.join(db_columns)

                    # Get values excluding FQDN
                    values = []
                    for col in columns_without_fqdn:
                        val = row[col]
                        if pd.api.types.is_datetime64_any_dtype(df[col]) and pd.notnull(val):
                            val = val.to_pydatetime()
                        elif pd.isna(val):
                            val = None
                        values.append(val)

                    query = f"""
                        INSERT INTO demo.{table} ({columns_str})
                        VALUES ({placeholders})
                    """
                    cur.execute(query, tuple(values))
                    pending_fqdn_count += 1
                else:
                    # Regular INSERT with all columns
                    columns = list(df.columns)
                    db_columns = [mapping.get(col, col) for col in columns]
                    placeholders = ', '.join(['%s'] * len(columns))
                    columns_str = ', '.join(db_columns)

                    # Get all values
                    values = []
                    for col in columns:
                        val = row[col]
                        if pd.api.types.is_datetime64_any_dtype(df[col]) and pd.notnull(val):
                            val = val.to_pydatetime()
                        elif pd.isna(val):
                            val = None
                        values.append(val)

                    query = f"""
                        INSERT INTO demo.{table} ({columns_str})
                        VALUES ({placeholders})
                    """
                    cur.execute(query, tuple(values))

                # Track quality grade
                grade = row.get('record_quality_grade', '_PT_YELLOW_RECORD_')
                if grade in quality_counts:
                    quality_counts[grade] += 1

            self.stats['rows_imported'][table] = len(df)
            self.stats['pending_fqdn_count'] = pending_fqdn_count
            self.stats['quality_counts'] = quality_counts

            print(f"  ‚úì Imported {table}: {len(df)} rows")
            if pending_fqdn_count > 0:
                print(f"  ‚Ñπ {pending_fqdn_count} components using auto-generated FQDNs")
            print(f"  üìä Quality: GREEN={quality_counts['_PT_GREEN_RECORD_']}, "
                  f"YELLOW={quality_counts['_PT_YELLOW_RECORD_']}, "
                  f"RED={quality_counts['_PT_RED_RECORD_']}")
        
        conn.commit()
    
    def import_relationships(self, conn: psycopg2.extensions.connection,
                           excel_data: Dict[str, pd.DataFrame]) -> None:
        """Import component relationships."""
        print("\nüì• Importing component relationships...")
        
        table = 'biz_component_relationships'
        if table not in excel_data:
            self.stats['warnings'].append(f"No {table} sheet found in Excel")
            return
        
        df = excel_data[table]
        if df.empty:
            self.stats['warnings'].append(f"No data in {table} sheet")
            return
        
        # Get column mapping
        mapping = self.get_column_mapping(table)
        
        with conn.cursor() as cur:
            # Prepare data
            columns = list(df.columns)
            db_columns = [mapping.get(col, col) for col in columns]
            
            # Build INSERT query
            placeholders = ', '.join(['%s'] * len(columns))
            columns_str = ', '.join(db_columns)
            query = f"""
                INSERT INTO demo.{table} ({columns_str})
                VALUES ({placeholders})
            """
            
            # Convert DataFrame to list of tuples, handling timestamps
            df_copy = df.copy()
            
            # Replace NaN with None
            df_copy = df_copy.where(pd.notnull(df_copy), None)
            
            # Convert to list of lists (not records to avoid numpy type issues)
            rows = []
            for idx, row in df_copy.iterrows():
                row_data = []
                for col in df_copy.columns:
                    val = row[col]
                    # Convert pandas timestamps to Python datetime
                    if pd.api.types.is_datetime64_any_dtype(df[col]) and pd.notnull(val):
                        val = val.to_pydatetime()
                    row_data.append(val)
                rows.append(tuple(row_data))
            
            # Validate foreign keys exist
            invalid_count = 0
            valid_rows = []
            
            for row in rows:
                row_dict = dict(zip(columns, row))
                
                # Check if component_id exists
                cur.execute("""
                    SELECT EXISTS(
                        SELECT 1 FROM demo.biz_components 
                        WHERE component_id = %s
                    )
                """, (row_dict.get('component_id'),))
                
                if not cur.fetchone()[0]:
                    invalid_count += 1
                    continue
                
                # Check if related_component_id exists
                cur.execute("""
                    SELECT EXISTS(
                        SELECT 1 FROM demo.biz_components 
                        WHERE component_id = %s
                    )
                """, (row_dict.get('related_component_id'),))
                
                if not cur.fetchone()[0]:
                    invalid_count += 1
                    continue
                
                valid_rows.append(row)
            
            if invalid_count > 0:
                self.stats['warnings'].append(
                    f"Skipped {invalid_count} relationships with invalid component IDs"
                )
            
            # Bulk insert valid rows
            if valid_rows:
                execute_batch(cur, query, valid_rows, page_size=100)
                self.stats['rows_imported'][table] = len(valid_rows)
                print(f"  ‚úì Imported {table}: {len(valid_rows)} rows")
                
                if invalid_count > 0:
                    print(f"  ‚ö† Skipped {invalid_count} invalid relationships")
        
        conn.commit()
    
    def verify_import(self, conn: psycopg2.extensions.connection) -> None:
        """Verify imported data."""
        print("\n‚úÖ Verifying import...")
        
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            # Count rows in each table
            verification = []
            
            for table in ['biz_components', 'biz_component_relationships']:
                cur.execute(f"SELECT COUNT(*) as count FROM demo.{table}")
                count = cur.fetchone()['count']
                
                excel_count = self.stats['rows_imported'].get(table, 0)
                status = '‚úì' if count == excel_count else '‚ö†'
                
                verification.append({
                    'Table': table,
                    'Database': count,
                    'Excel': excel_count,
                    'Status': status
                })
            
            # Check for orphaned relationships
            cur.execute("""
                SELECT COUNT(*) as count
                FROM demo.biz_component_relationships r
                WHERE NOT EXISTS (
                    SELECT 1 FROM demo.biz_components c 
                    WHERE c.component_id = r.component_id
                )
                OR NOT EXISTS (
                    SELECT 1 FROM demo.biz_components c 
                    WHERE c.component_id = r.related_component_id
                )
            """)
            orphaned = cur.fetchone()['count']
            
            if orphaned > 0:
                self.stats['warnings'].append(f"Found {orphaned} orphaned relationships")
            
            # Print verification table
            print("\n" + tabulate(verification, headers='keys', tablefmt='grid'))
    
    def print_summary(self) -> None:
        """Print import summary with data quality statistics."""
        print("\n" + "="*60)
        print("IMPORT SUMMARY")
        print("="*60)

        print(f"\nüìÅ Source: {self.excel_path.name}")
        print(f"üìÖ Imported: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        print("\nüìä Tables Imported:")
        for table, count in self.stats['rows_imported'].items():
            print(f"  ‚Ä¢ {table}: {count:,} rows")

        total_rows = sum(self.stats['rows_imported'].values())
        print(f"\n  Total: {total_rows:,} rows")

        # Data quality statistics
        if 'quality_counts' in self.stats:
            print("\nüìà Data Quality Distribution:")
            quality = self.stats['quality_counts']
            print(f"  üü¢ Complete (GREEN): {quality.get('_PT_GREEN_RECORD_', 0)} components")
            print(f"  üü° Partial (YELLOW): {quality.get('_PT_YELLOW_RECORD_', 0)} components")
            print(f"  üî¥ Minimal (RED): {quality.get('_PT_RED_RECORD_', 0)} components")

        if 'pending_fqdn_count' in self.stats and self.stats['pending_fqdn_count'] > 0:
            print(f"\nüîÑ Pending Discovery:")
            print(f"  ‚Ä¢ {self.stats['pending_fqdn_count']} components using auto-generated FQDNs")
        
        if self.stats['warnings']:
            print("\n‚ö†Ô∏è  Warnings:")
            for warning in self.stats['warnings']:
                print(f"  ‚Ä¢ {warning}")
        
        if self.stats['errors']:
            print("\n‚ùå Errors:")
            for error in self.stats['errors']:
                print(f"  ‚Ä¢ {error}")
        else:
            print("\n‚úÖ Import completed successfully!")
    
    def run(self, skip_confirm: bool = False) -> bool:
        """Execute the import process."""
        try:
            # Confirm before nuking
            if not skip_confirm:
                print("\n" + "="*60)
                print("‚ö†Ô∏è  WARNING: This will DELETE ALL DATA in the demo schema!")
                print(f"üìÅ Importing from: {self.excel_path.name}")
                print("="*60)
                
                response = input("\nProceed? (yes/no): ").strip().lower()
                if response != 'yes':
                    print("Import cancelled.")
                    return False
            
            # Connect to database
            conn = self.connect()
            
            try:
                # Introspect schema
                schema_info = self.introspect_schema(conn)
                print(f"\nüìã Found {len(schema_info)} tables in demo schema")
                
                # Read Excel data
                excel_data = self.read_excel_data()
                
                # Nuke existing data
                self.nuke_demo_schema(conn)
                
                # Ensure reference data exists (from Excel or live schema)
                self.ensure_reference_data(conn, excel_data)
                
                # Import business data
                self.import_components(conn, excel_data)
                self.import_relationships(conn, excel_data)
                
                # Verify
                self.verify_import(conn)
                
                # Print summary
                self.print_summary()
                
                return True
                
            finally:
                conn.close()
                
        except Exception as e:
            self.stats['errors'].append(str(e))
            print(f"\n‚ùå Import failed: {e}")
            return False


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description='Import demo data from Excel into PostgreSQL demo schema'
    )
    parser.add_argument(
        '--file',
        help='Path to Excel file (default: latest in spreadsheets.d/)'
    )
    parser.add_argument(
        '--confirm',
        action='store_true',
        help='Skip confirmation prompt'
    )
    
    args = parser.parse_args()
    
    # Load environment and display connection info
    load_dotenv()
    
    # Check for database URL
    connection_url = os.environ.get('POSTGRESQL_BAIS_DB_ADMIN_URL')
    if connection_url:
        # Display connection information
        from urllib.parse import urlparse
        parsed = urlparse(connection_url)
        print(f"{'='*60}")
        print("DATABASE CONNECTION INFO")
        print(f"{'='*60}")
        print(f"  Host:     {parsed.hostname}")
        print(f"  Port:     {parsed.port}")
        print(f"  Database: {parsed.path.lstrip('/')}")
        print(f"  User:     {parsed.username}")
        print(f"  Password: {'*' * 8 if parsed.password else 'not set'}")
        print(f"  Schema:   demo")
        print(f"{'='*60}")
        print()
    else:
        # Display from individual env vars
        print(f"{'='*60}")
        print("DATABASE CONNECTION INFO")
        print(f"{'='*60}")
        print(f"  Host:     {os.getenv('POSTGRESQL_INSTANCE_HOST', 'localhost')}")
        print(f"  Port:     {os.getenv('POSTGRESQL_INSTANCE_PORT', '5432')}")
        print(f"  Database: {os.getenv('POSTGRESQL_BAIS_DB', 'prutech_bais')}")
        print(f"  User:     {os.getenv('POSTGRESQL_BAIS_DB_ADMIN_USER', 'not set')}")
        print(f"  Password: {'*' * 8 if os.getenv('POSTGRESQL_BAIS_DB_ADMIN_PASSWORD') else 'not set'}")
        print(f"  Schema:   demo")
        print(f"{'='*60}")
        print()
    
    # Run import
    importer = DemoDataImporter(args.file)
    success = importer.run(skip_confirm=args.confirm)
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
